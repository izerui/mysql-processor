import os
import shutil

import threading
import time


from base import BaseShell, Mysql
from logger_config import logger


class MyDump(BaseShell):
    """
    ä½¿ç”¨mysqldumpå¯¼å‡ºæ•°æ®åº“å¤‡ä»½ï¼Œæ”¯æŒpvè¿›åº¦æ˜¾ç¤ºå’Œæ–‡ä»¶å¤§å°ç›‘æ§
    """

    def __init__(self, mysql: Mysql):
        super().__init__()
        self.mysql = mysql
        self.use_pv = self._check_pv_available()

    def _check_pv_available(self):
        """æ£€æŸ¥pvå·¥å…·æ˜¯å¦å¯ç”¨"""
        return shutil.which('pv') is not None

    def _monitor_progress(self, dump_dir, interval=2, stop_event=None):
        """
        ç›‘æ§å¯¼å‡ºç›®å½•ä¸­æ‰€æœ‰æ–‡ä»¶çš„æ€»å¤§å°å’Œå¯¼å‡ºé€Ÿåº¦
        :param dump_dir: å¯¼å‡ºçš„ç›®å½•è·¯å¾„
        :param interval: ç›‘æ§é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰
        :param stop_event: ç”¨äºé€šçŸ¥ç›‘æ§çº¿ç¨‹åœæ­¢çš„äº‹ä»¶
        """
        last_total_size = 0
        last_check_time = time.time()
        no_change_count = 0
        max_no_change = 3  # è¿ç»­3æ¬¡æ— å˜åŒ–è®¤ä¸ºå¯¼å‡ºå®Œæˆ
        max_wait_cycles = 300  # æœ€å¤šç­‰å¾…300ä¸ªå‘¨æœŸï¼ˆçº¦10åˆ†é’Ÿï¼‰ï¼Œé˜²æ­¢æ— é™è¿è¡Œ

        cycle_count = 0
        while cycle_count < max_wait_cycles:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢ç›‘æ§
            if stop_event and stop_event.is_set():
                logger.info("ğŸ›‘ ç›‘æ§çº¿ç¨‹æ”¶åˆ°åœæ­¢ä¿¡å·")
                break

            try:
                if os.path.exists(dump_dir):
                    total_size = 0
                    file_count = 0

                    # è®¡ç®—ç›®å½•ä¸­æ‰€æœ‰.sqlæ–‡ä»¶çš„æ€»å¤§å°
                    for file in os.listdir(dump_dir):
                        if file.endswith('.sql'):
                            file_path = os.path.join(dump_dir, file)
                            if os.path.exists(file_path):
                                total_size += os.path.getsize(file_path)
                                file_count += 1

                    total_size_mb = total_size / (1024 * 1024)
                    current_time = time.time()
                    time_elapsed = current_time - last_check_time

                    if total_size != last_total_size:
                        # è®¡ç®—é€Ÿåº¦
                        size_diff = total_size - last_total_size
                        speed_mbps = (size_diff / (1024 * 1024)) / time_elapsed if time_elapsed > 0 else 0

                        if speed_mbps > 0:
                            logger.info("ğŸ“Š å¯¼å‡ºè¿›åº¦: {} ä¸ªæ–‡ä»¶ï¼Œæ€»å¤§å°: {:.2f} MBï¼Œé€Ÿåº¦: {:.2f} MB/s".format(
                                file_count, total_size_mb, speed_mbps))
                        else:
                            logger.info("ğŸ“Š å¯¼å‡ºè¿›åº¦: {} ä¸ªæ–‡ä»¶ï¼Œæ€»å¤§å°: {:.2f} MB".format(file_count, total_size_mb))

                        last_total_size = total_size
                        last_check_time = current_time
                        no_change_count = 0  # é‡ç½®è®¡æ•°å™¨
                    else:
                        no_change_count += 1
                        if no_change_count >= max_no_change:
                            # æ–‡ä»¶å¤§å°è¿ç»­å¤šæ¬¡æ— å˜åŒ–ï¼Œè®¤ä¸ºå¯¼å‡ºå®Œæˆ
                            if total_size > 0:
                                # è®¡ç®—æ€»è€—æ—¶å’Œå¹³å‡é€Ÿåº¦
                                total_time = time.time() - last_check_time + (no_change_count * interval)
                                avg_speed = total_size_mb / total_time if total_time > 0 else 0
                                logger.info("âœ… å¯¼å‡ºå®Œæˆ: {} ä¸ªæ–‡ä»¶ï¼Œæ€»å¤§å°: {:.2f} MBï¼Œå¹³å‡é€Ÿåº¦: {:.2f} MB/s".format(
                                    file_count, total_size_mb, avg_speed))
                            break
                else:
                    # ç›®å½•è¿˜ä¸å­˜åœ¨ï¼Œç­‰å¾…åˆ›å»º
                    no_change_count += 1
                    if no_change_count >= max_no_change:
                        break
            except Exception as e:
                logger.error("ç›‘æ§è¿›åº¦æ—¶å‡ºé”™: {}".format(str(e)))
                break

            cycle_count += 1
            time.sleep(interval)

        if cycle_count >= max_wait_cycles:
            logger.warning("âš ï¸ ç›‘æ§çº¿ç¨‹è¶…æ—¶åœæ­¢ï¼Œå¯èƒ½å¯¼å‡ºè¿‡ç¨‹å¼‚å¸¸")

    def export_dbs(self, databases, dump_file, tables=None):
        """
        ä½¿ç”¨mysqldumpå¯¼å‡ºæ•°æ®åº“åˆ°SQLæ–‡ä»¶
        :param databases: æ•°æ®åº“åˆ—è¡¨
        :param dump_file: å¯¼å‡ºçš„SQLæ–‡ä»¶è·¯å¾„
        :param tables: è¡¨ååˆ—è¡¨ï¼Œé»˜è®¤ä¸ºNoneå¯¼å‡ºæ‰€æœ‰è¡¨
        :return:
        """
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(dump_file), exist_ok=True)

            mysqldump_path = self._get_mysqldump_exe()

            # æ„å»ºmysqldumpå‘½ä»¤ï¼Œä½¿ç”¨å®Œæ•´è·¯å¾„
            if tables and tables != ['*']:
                # æŒ‡å®šè¡¨å¯¼å‡ºï¼šåªæ”¯æŒå•ä¸ªæ•°æ®åº“
                if len(databases) > 1:
                    raise ValueError("æŒ‡å®šè¡¨å¯¼å‡ºæ—¶åªèƒ½é€‰æ‹©ä¸€ä¸ªæ•°æ®åº“")
                database = databases[0]
                cmd = '{} -h {} -u {} -p{} --port={} --default-character-set=utf8 --set-gtid-purged=OFF --skip-routines --skip-triggers --skip-add-locks --disable-keys --skip-events --skip-set-charset --compact --add-drop-database --extended-insert --complete-insert --quick --skip-lock-tables --no-autocommit --compress --skip-tz-utc --max-allowed-packet=256M --net-buffer-length=65536 {} {}'.format(
                    mysqldump_path, self.mysql.db_host, self.mysql.db_user, "'{}'".format(self.mysql.db_pass),
                    self.mysql.db_port, database, " ".join(tables))
            else:
                # å®Œæ•´æ•°æ®åº“å¯¼å‡º
                cmd = '{} -h {} -u {} -p{} --port={} --default-character-set=utf8 --set-gtid-purged=OFF --skip-routines --skip-triggers --skip-add-locks --disable-keys --skip-events --skip-set-charset --compact --add-drop-database --extended-insert --complete-insert --quick --skip-lock-tables --no-autocommit --compress --skip-tz-utc --max-allowed-packet=256M --net-buffer-length=65536 --databases {}'.format(
                    mysqldump_path, self.mysql.db_host, self.mysql.db_user, "'{}'".format(self.mysql.db_pass),
                    self.mysql.db_port, " ".join(databases))

            # è·å–mysqldumpçš„binç›®å½•ä½œä¸ºå·¥ä½œç›®å½•
            mysqldump_bin_dir = os.path.dirname(mysqldump_path)

            # ä½¿ç”¨æ ‡å‡†mysqldumpå‘½ä»¤ï¼ˆæš‚æ—¶ç§»é™¤pvï¼‰
            full_command = '{} > {}'.format(cmd, dump_file)
            logger.info("æ­£åœ¨å¯¼å‡ºæ•°æ®åº“...")

            # å¯åŠ¨è¿›åº¦ç›‘æ§çº¿ç¨‹ï¼Œç›‘æ§æ•´ä¸ªå¯¼å‡ºç›®å½•
            dump_dir = os.path.dirname(dump_file)
            stop_event = threading.Event()
            progress_thread = threading.Thread(
                target=self._monitor_progress,
                args=(dump_dir, 2, stop_event),
                daemon=True
            )
            progress_thread.start()

            try:
                # ä½¿ç”¨BaseShellçš„_exe_commandæ–¹æ³•æ‰§è¡Œå‘½ä»¤
                success, exit_code, output = self._exe_command(
                    full_command,
                    cwd=mysqldump_bin_dir
                )

                if not success:
                    stop_event.set()  # é€šçŸ¥ç›‘æ§çº¿ç¨‹åœæ­¢
                    raise RuntimeError("mysqldumpå¯¼å‡ºå¤±è´¥ï¼Œexit code: {}".format(exit_code))

                # ç­‰å¾…ç›‘æ§çº¿ç¨‹å®Œæˆï¼ˆæœ€å¤šç­‰å¾…5ç§’ï¼‰
                progress_thread.join(timeout=5)
                logger.info('âœ… å‘½ä»¤æ‰§è¡ŒæˆåŠŸ')
                return True

            except Exception as e:
                # ç¡®ä¿åœ¨ä»»ä½•å¼‚å¸¸æƒ…å†µä¸‹éƒ½åœæ­¢ç›‘æ§çº¿ç¨‹
                stop_event.set()
                if progress_thread.is_alive():
                    progress_thread.join(timeout=2)
                raise e

        except RuntimeError as e:
            raise e
        except Exception as e:
            raise RuntimeError("å¯¼å‡ºè¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {}".format(str(e)))
