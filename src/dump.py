import os
import shutil
import time
import concurrent.futures
import re
import configparser
from typing import List, Optional

from base import BaseShell, Mysql
from logger_config import logger


class MyDump(BaseShell):
    """
    ä½¿ç”¨mysqldumpå¯¼å‡ºæ•°æ®åº“å¤‡ä»½ - é‡æ„ç‰ˆ
    æä¾›æ¸…æ™°çš„è¿›åº¦æ˜¾ç¤ºå’Œç»“æ„åŒ–æ—¥å¿—
    """

    def __init__(self, mysql: Mysql):
        super().__init__()
        self.mysql = mysql
        self.use_pv = self._check_pv_available()
        self.split_threshold = self._get_split_threshold()

    def _check_pv_available(self):
        """æ£€æŸ¥pvå·¥å…·æ˜¯å¦å¯ç”¨"""
        return shutil.which('pv') is not None

    def _get_split_threshold(self):
        """ä»é…ç½®æ–‡ä»¶è¯»å–æ–‡ä»¶æ‹†åˆ†é˜ˆå€¼"""
        try:
            config = configparser.ConfigParser()
            config_path = os.path.join(os.path.dirname(__file__), '..', 'config.ini')
            config.read(config_path, encoding='utf-8')
            threshold = config.getint('global', 'split_threshold', fallback=500)
            return threshold * 1024 * 1024  # è½¬æ¢ä¸ºå­—èŠ‚
        except Exception:
            return 500 * 1024 * 1024  # é»˜è®¤500MB

    def export_db(self, database: str, dump_file: str, tables: Optional[List[str]] = None):
        """
        ä½¿ç”¨mysqldumpå¯¼å‡ºæ•°æ®åº“ç»“æ„ï¼Œç„¶åä½¿ç”¨çº¿ç¨‹æ± åˆ†åˆ«å¯¼å‡ºæ¯ä¸ªè¡¨çš„æ•°æ®
        æä¾›æ¸…æ™°çš„è¿›åº¦æ˜¾ç¤º
        """
        start_time = time.time()
        logger.log_database_start(database, "å¯¼å‡º")

        try:
            # æ¸…ç†å·²å­˜åœ¨çš„æ–‡ä»¶å’Œç›®å½•
            self._cleanup_existing_files(dump_file, database)

            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(dump_file), exist_ok=True)

            mysqldump_path = self._get_mysqldump_exe()
            mysqldump_bin_dir = os.path.dirname(mysqldump_path)

            # ç¬¬ä¸€æ­¥ï¼šå¯¼å‡ºæ•°æ®åº“ç»“æ„
            logger.info(f"ğŸ“Š æ­£åœ¨å¯¼å‡ºæ•°æ®åº“ç»“æ„...")
            structure_start = time.time()
            if not self._export_structure(database, dump_file, mysqldump_path, mysqldump_bin_dir):
                return False

            # ç¬¬äºŒæ­¥ï¼šè·å–æ•°æ®åº“çš„æ‰€æœ‰è¡¨
            if tables is None or tables == ['*']:
                tables = self._get_all_tables(database)

            if not tables:
                logger.info(f"â„¹ï¸ æ•°æ®åº“ {database} ä¸­æ²¡æœ‰è¡¨éœ€è¦å¯¼å‡ºæ•°æ®")
                logger.log_database_complete(database, "å¯¼å‡º", time.time() - start_time)
                return True

            # ç¬¬ä¸‰æ­¥ï¼šå¯¼å‡ºè¡¨æ•°æ®
            logger.info(f"ğŸ“Š å‘ç° {len(tables)} ä¸ªè¡¨éœ€è¦å¯¼å‡ºæ•°æ®")
            success_count = self._export_tables_data(database, tables, dump_file, mysqldump_path, mysqldump_bin_dir)

            if success_count == len(tables):
                total_duration = time.time() - start_time
                logger.log_database_complete(database, "å¯¼å‡º", total_duration)
                return True
            else:
                logger.error(f"å¯¼å‡ºå¤±è´¥: {len(tables) - success_count} ä¸ªè¡¨å¯¼å‡ºå¤±è´¥")
                return False

        except Exception as e:
            logger.error(f"å¯¼å‡ºè¿‡ç¨‹å‘ç”Ÿé”™è¯¯ - æ•°æ®åº“: {database}, é”™è¯¯: {str(e)}")
            return False

    def _cleanup_existing_files(self, dump_file: str, database: str):
        """æ¸…ç†å·²å­˜åœ¨çš„æ–‡ä»¶å’Œç›®å½•"""
        # åˆ é™¤å·²å­˜åœ¨çš„æ•°æ®åº“ç»“æ„æ–‡ä»¶
        if os.path.exists(dump_file):
            os.remove(dump_file)
            logger.cleanup(f"æ•°æ®åº“ç»“æ„æ–‡ä»¶: {dump_file}")

        # åˆ é™¤å·²å­˜åœ¨çš„æ•°æ®åº“æ–‡ä»¶å¤¹
        db_folder = os.path.join(os.path.dirname(dump_file), database)
        if os.path.exists(db_folder):
            shutil.rmtree(db_folder)
            logger.cleanup(f"æ•°æ®åº“æ–‡ä»¶å¤¹: {db_folder}")

    def _export_structure(self, database: str, dump_file: str, mysqldump_path: str, mysqldump_bin_dir: str) -> bool:
        """å¯¼å‡ºæ•°æ®åº“ç»“æ„"""
        try:
            cmd = (
                f'{mysqldump_path} '
                f'-h {self.mysql.db_host} '
                f'-u {self.mysql.db_user} '
                f'-p"{self.mysql.db_pass}" '
                f'--port={self.mysql.db_port} '
                f'--default-character-set=utf8 '
                f'--set-gtid-purged=OFF '
                f'--skip-routines '
                f'--skip-triggers '
                f'--skip-add-locks '
                f'--disable-keys '
                f'--skip-events '
                f'--skip-set-charset '
                f'--add-drop-database '
                f'--extended-insert '
                f'--complete-insert '
                f'--quick '
                f'--no-autocommit '
                f'--single-transaction '
                f'--skip-lock-tables '
                f'--no-autocommit '
                f'--compress '
                f'--skip-tz-utc '
                f'--max-allowed-packet=256M '
                f'--net-buffer-length=1048576 '
                f'--no-data '
                f'--skip-set-charset '
                f'--skip-comments '
                f'--compact '
                f'--databases {database}'
            )

            full_command = f'{cmd} > {dump_file}'
            success, exit_code, output = self._exe_command(full_command, cwd=mysqldump_bin_dir)

            if not success:
                raise RuntimeError(f"æ•°æ®åº“ç»“æ„å¯¼å‡ºå¤±è´¥ï¼Œexit code: {exit_code}")

            file_size = os.path.getsize(dump_file) / 1024 / 1024
            logger.success(f"æ•°æ®åº“ç»“æ„å¯¼å‡ºå®Œæˆ ({file_size:.1f}MB)")
            return True

        except Exception as e:
            logger.error(f"æ•°æ®åº“ç»“æ„å¯¼å‡ºå¤±è´¥ - æ•°æ®åº“: {database}, é”™è¯¯: {str(e)}")
            return False

    def _export_tables_data(self, database: str, tables: List[str], dump_file: str,
                          mysqldump_path: str, mysqldump_bin_dir: str) -> int:
        """å¹¶å‘å¯¼å‡ºæ‰€æœ‰è¡¨çš„æ•°æ®"""
        db_folder = os.path.join(os.path.dirname(dump_file), database)
        os.makedirs(db_folder, exist_ok=True)

        logger.info(f"ğŸ”„ å¼€å§‹å¹¶å‘å¯¼å‡ºè¡¨æ•°æ®...")
        export_start = time.time()

        success_count = 0
        failed_tables = []

        with concurrent.futures.ThreadPoolExecutor(max_workers=8) as pool:
            # æäº¤æ‰€æœ‰å¯¼å‡ºä»»åŠ¡
            futures = []
            for idx, table in enumerate(tables):
                table_file = os.path.join(db_folder, f"{table}.sql")
                future = pool.submit(
                    self._export_single_table,
                    database, table, table_file,
                    mysqldump_path, mysqldump_bin_dir,
                    idx + 1, len(tables)
                )
                futures.append((table, future))

            # æ”¶é›†ç»“æœ
            for table, future in futures:
                try:
                    result = future.result()
                    if result['success']:
                        success_count += 1
                        logger.log_table_complete(
                            database, table, result['duration'], result['size_mb']
                        )
                    else:
                        failed_tables.append(table)
                        logger.error(f"è¡¨å¯¼å‡ºå¤±è´¥ - æ•°æ®åº“: {database}, è¡¨: {table}, é”™è¯¯: {result['error']}")
                except Exception as e:
                    failed_tables.append(table)
                    logger.error(f"è¡¨å¯¼å‡ºå¼‚å¸¸ - æ•°æ®åº“: {database}, è¡¨: {table}, é”™è¯¯: {str(e)}")

                # æ›´æ–°æ‰¹é‡è¿›åº¦
                progress = (success_count + len(failed_tables)) / len(tables) * 100
                logger.log_batch_progress(
                    "è¡¨æ•°æ®å¯¼å‡º",
                    success_count + len(failed_tables),
                    len(tables),
                    len(failed_tables)
                )

        export_duration = time.time() - export_start
        logger.info(f"è¡¨æ•°æ®å¯¼å‡ºç»Ÿè®¡ - æˆåŠŸ: {success_count}, å¤±è´¥: {len(failed_tables)}, æ€»è®¡: {len(tables)}, è€—æ—¶: {export_duration:.1f}s")

        return success_count

    def _export_single_table(self, database: str, table: str, table_file: str,
                           mysqldump_path: str, mysqldump_bin_dir: str,
                           current_num: int, total_tables: int) -> dict:
        """å¯¼å‡ºå•ä¸ªè¡¨çš„æ•°æ®"""
        start_time = time.time()

        try:
            cmd = (
                f'{mysqldump_path} '
                f'-h {self.mysql.db_host} '
                f'-u {self.mysql.db_user} '
                f'-p"{self.mysql.db_pass}" '
                f'--port={self.mysql.db_port} '
                f'--default-character-set=utf8 '
                f'--set-gtid-purged=OFF '
                f'--skip-routines '
                f'--skip-triggers '
                f'--skip-add-locks '
                f'--disable-keys '
                f'--skip-events '
                f'--skip-set-charset '
                f'--extended-insert '
                f'--complete-insert '
                f'--quick '
                f'--no-autocommit '
                f'--single-transaction '
                f'--skip-lock-tables '
                f'--no-autocommit '
                f'--compress '
                f'--skip-tz-utc '
                f'--max-allowed-packet=256M '
                f'--net-buffer-length=1048576 '
                f'--no-create-info '
                f'--skip-set-charset '
                f'--skip-comments '
                f'--compact '
                f'{database} {table}'
            )

            # å…ˆå¯¼å‡ºåˆ°ä¸´æ—¶æ–‡ä»¶
            temp_file = f"{table_file}.tmp"
            full_command = f'{cmd} > {temp_file}'

            success, exit_code, output = self._exe_command(
                full_command, cwd=mysqldump_bin_dir
            )

            if not success:
                raise RuntimeError(f"è¡¨æ•°æ®å¯¼å‡ºå¤±è´¥ï¼Œexit code: {exit_code}")

            # å¤„ç†æ–‡ä»¶
            if os.path.exists(temp_file):
                file_size = os.path.getsize(temp_file)

                if file_size > self.split_threshold:
                    # å¤§æ–‡ä»¶éœ€è¦æ‹†åˆ†
                    file_size_mb = file_size / 1024 / 1024
                    logger.info(
                        f"æ–‡ä»¶è¿‡å¤§ï¼Œæ­£åœ¨æ‹†åˆ†",
                        {"table": table, "size": f"{file_size_mb:.1f}MB"}
                    )
                    self._split_large_file(temp_file, table_file, self.split_threshold)
                    os.remove(temp_file)
                    # æ–‡ä»¶å·²æ‹†åˆ†ï¼Œä½¿ç”¨åŸå§‹æ–‡ä»¶å¤§å°ä½œä¸ºå‚è€ƒ
                    file_size_mb = file_size / 1024 / 1024
                else:
                    # å°æ–‡ä»¶ç›´æ¥é‡å‘½å
                    os.rename(temp_file, table_file)
                    file_size_mb = os.path.getsize(table_file) / 1024 / 1024
                return {
                    'success': True,
                    'duration': time.time() - start_time,
                    'size_mb': file_size_mb
                }
            else:
                # ç©ºæ–‡ä»¶
                open(table_file, 'w').close()
                return {
                    'success': True,
                    'duration': time.time() - start_time,
                    'size_mb': 0
                }

        except Exception as e:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_file):
                os.remove(temp_file)
            return {
                'success': False,
                'duration': time.time() - start_time,
                'error': str(e)
            }

    def _get_all_tables(self, database: str) -> List[str]:
        """è·å–æ•°æ®åº“ä¸­çš„æ‰€æœ‰è¡¨å"""
        try:
            import pymysql
            connection = pymysql.connect(
                host=self.mysql.db_host,
                user=self.mysql.db_user,
                password=self.mysql.db_pass,
                port=int(self.mysql.db_port),
                database=database,
                charset='utf8'
            )

            with connection.cursor() as cursor:
                cursor.execute("SHOW TABLES")
                tables = [row[0] for row in cursor.fetchall()]

            connection.close()
            logger.info(f"ğŸ“Š è·å–è¡¨åˆ—è¡¨å®Œæˆ - æ•°æ®åº“: {database}, è¡¨æ•°é‡: {len(tables)}")
            return sorted(tables)

        except Exception as e:
            logger.error(f"è·å–è¡¨åˆ—è¡¨å¤±è´¥ - æ•°æ®åº“: {database}, é”™è¯¯: {str(e)}")
            return []

    def _split_large_file(self, temp_file: str, base_filename: str, max_size: int):
        """å°†å¤§æ–‡ä»¶æŒ‰æŒ‡å®šå¤§å°æ‹†åˆ†æˆå¤šä¸ªæ–‡ä»¶"""
        try:
            file_number = 1
            current_size = 0
            current_file = None

            total_size = os.path.getsize(temp_file)
            processed_size = 0

            with open(temp_file, 'r', encoding='utf-8') as f:
                line_buffer = []
                buffer_size_bytes = 0

                for line in f:
                    line_bytes = line.encode('utf-8')
                    line_size = len(line_bytes)
                    processed_size += line_size

                    # è¿›åº¦æ˜¾ç¤º
                    if processed_size % (10 * 1024 * 1024) < line_size:  # æ¯10MBæ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                        progress = (processed_size / total_size) * 100
                        logger.log_table_progress(
                            os.path.basename(base_filename).split('.')[0],
                            f"æ‹†åˆ†è¿›åº¦",
                            progress,
                            processed_size // 1024 // 1024,
                            total_size // 1024 // 1024
                        )

                    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ–°æ–‡ä»¶
                    if line.strip().startswith('INSERT INTO'):
                        if current_file and current_size + buffer_size_bytes + line_size > max_size:
                            current_file.write(''.join(line_buffer))
                            line_buffer = []
                            buffer_size_bytes = 0
                            current_file.close()
                            file_number += 1
                            current_file = None
                            current_size = 0

                        if current_file is None:
                            base_name_without_ext = os.path.splitext(base_filename)[0]
                            ext = os.path.splitext(base_filename)[1]
                            current_file = open(
                                f"{base_name_without_ext}.part{file_number:03d}{ext}",
                                'w', encoding='utf-8'
                            )
                            current_size = 0

                    line_buffer.append(line)
                    buffer_size_bytes += line_size

                    if buffer_size_bytes >= 1024 * 1024:  # 1MBæ—¶å†™å…¥
                        if current_file:
                            current_file.write(''.join(line_buffer))
                            current_size += buffer_size_bytes
                        line_buffer = []
                        buffer_size_bytes = 0

                # å†™å…¥å‰©ä½™å†…å®¹
                if line_buffer and current_file:
                    current_file.write(''.join(line_buffer))
                    current_file.close()
                elif current_file:
                    current_file.close()

            # æ¸…é™¤è¿›åº¦æ¡
            print(f"\r{' ' * 100}\r", end="")
            logger.info(f"æ–‡ä»¶æ‹†åˆ†å®Œæˆ - æ–‡ä»¶æ•°: {file_number}, æ€»å¤§å°: {total_size/1024/1024:.1f}MB")

        except Exception as e:
            logger.error(f"æ‹†åˆ†æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            raise
